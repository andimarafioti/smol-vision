{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"SmolVLM_Benchmarks.ipynb\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1xrpj5QUTActxwIqh3SVfJwUPp6VLwTt7\n",
    "\"\"\"\n",
    "\n",
    "# !pip install -q bitsandbytes accelerate quanto optimum-quanto\n",
    "\n",
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()\n",
    "\n",
    "from transformers import AutoProcessor, QuantoConfig, AutoModelForVision2Seq, AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
    "import time\n",
    "import gc\n",
    "import requests\n",
    "from PIL import Image\n",
    "import time\n",
    "import torch\n",
    "import requests\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "class InternVL2Processor:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"OpenGVLab/InternVL2-2B\", trust_remote_code=True, use_fast=False)\n",
    "\n",
    "    def build_transform(self, input_size):\n",
    "        MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "        transform = T.Compose([\n",
    "            T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "            T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=MEAN, std=STD)\n",
    "        ])\n",
    "        return transform\n",
    "\n",
    "    def find_closest_aspect_ratio(self, aspect_ratio, target_ratios, width, height, image_size):\n",
    "        best_ratio_diff = float('inf')\n",
    "        best_ratio = (1, 1)\n",
    "        area = width * height\n",
    "        for ratio in target_ratios:\n",
    "            target_aspect_ratio = ratio[0] / ratio[1]\n",
    "            ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "            if ratio_diff < best_ratio_diff:\n",
    "                best_ratio_diff = ratio_diff\n",
    "                best_ratio = ratio\n",
    "            elif ratio_diff == best_ratio_diff:\n",
    "                if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                    best_ratio = ratio\n",
    "        return best_ratio\n",
    "\n",
    "    def dynamic_preprocess(self, image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "        orig_width, orig_height = image.size\n",
    "        aspect_ratio = orig_width / orig_height\n",
    "\n",
    "        # calculate the existing image aspect ratio\n",
    "        target_ratios = set(\n",
    "            (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "            i * j <= max_num and i * j >= min_num)\n",
    "        target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "        # find the closest aspect ratio to the target\n",
    "        target_aspect_ratio = self.find_closest_aspect_ratio(\n",
    "            aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "        # calculate the target width and height\n",
    "        target_width = image_size * target_aspect_ratio[0]\n",
    "        target_height = image_size * target_aspect_ratio[1]\n",
    "        blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "        # resize the image\n",
    "        resized_img = image.resize((target_width, target_height))\n",
    "        processed_images = []\n",
    "        for i in range(blocks):\n",
    "            box = (\n",
    "                (i % (target_width // image_size)) * image_size,\n",
    "                (i // (target_width // image_size)) * image_size,\n",
    "                ((i % (target_width // image_size)) + 1) * image_size,\n",
    "                ((i // (target_width // image_size)) + 1) * image_size\n",
    "            )\n",
    "            # split the image\n",
    "            split_img = resized_img.crop(box)\n",
    "            processed_images.append(split_img)\n",
    "        assert len(processed_images) == blocks\n",
    "        if use_thumbnail and len(processed_images) != 1:\n",
    "            thumbnail_img = image.resize((image_size, image_size))\n",
    "            processed_images.append(thumbnail_img)\n",
    "        return processed_images\n",
    "\n",
    "    def load_image(self, image_url, input_size=448, max_num=12):\n",
    "        image = Image.open(requests.get(image_url, stream=True).raw).convert('RGB')\n",
    "        transform = self.build_transform(input_size=input_size)\n",
    "        images = self.dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "        pixel_values = [transform(image) for image in images]\n",
    "        pixel_values = torch.stack(pixel_values)\n",
    "        return pixel_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_memory():\n",
    "    \"\"\"Returns GPU memory usage in GB\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.memory_allocated() / 1024**3\n",
    "    return 0\n",
    "\n",
    "def load_model(model_id):\n",
    "    \"\"\"\n",
    "    Loads the model and measures memory usage.\n",
    "    Returns the model, processor, and base memory usage.\n",
    "    \"\"\"\n",
    "    # Clear any existing memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    initial_memory = get_gpu_memory()\n",
    "    print(f\"Initial GPU memory usage before loading: {initial_memory:.2f} GB\")\n",
    "    \n",
    "    if \"moondream\" in model_id.lower():\n",
    "        # Special case for Moondream\n",
    "        processor = AutoTokenizer.from_pretrained(model_id, revision=\"2024-08-26\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            trust_remote_code=True,\n",
    "            revision=\"2024-08-26\",\n",
    "            device_map=\"cuda:0\",\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "    elif 'internvl2' in model_id.lower():\n",
    "        model = AutoModel.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            use_flash_attn=True,\n",
    "            trust_remote_code=True).eval().cuda()\n",
    "        processor = InternVL2Processor()\n",
    "    elif 'minicpm' in model_id.lower():\n",
    "        model = AutoModel.from_pretrained('openbmb/MiniCPM-V-2', trust_remote_code=True, torch_dtype=torch.bfloat16)\n",
    "        model = model.to(device='cuda', dtype=torch.bfloat16)\n",
    "        processor = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-2', trust_remote_code=True)\n",
    "        model.eval()\n",
    "    else:\n",
    "        # Original code for other models\n",
    "        processor = AutoProcessor.from_pretrained(model_id)\n",
    "        model = AutoModelForVision2Seq.from_pretrained(\n",
    "            model_id,\n",
    "            _attn_implementation=\"flash_attention_2\",\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"cuda:0\",\n",
    "        )\n",
    "    \n",
    "    \n",
    "    base_memory = get_gpu_memory()\n",
    "    print(f\"GPU memory usage after loading: {base_memory:.2f} GB\")\n",
    "    print(f\"GPU memory usage from loading the model: {base_memory-initial_memory:.2f} GB\")\n",
    "    \n",
    "    return model, processor, base_memory\n",
    "\n",
    "def benchmark_vlm(model_id, batch_size):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    model, processor, base_memory = load_model(model_id)\n",
    "\n",
    "    image_url = \"https://huggingface.co/spaces/merve/chameleon-7b/resolve/main/bee.jpg\"\n",
    "    image = [Image.open(requests.get(image_url, stream=True).raw).convert('RGB')]\n",
    "\n",
    "    images = []\n",
    "    prompts = []\n",
    "    num_patches_list = []\n",
    "    pixel_values = None\n",
    "\n",
    "    for _ in range(batch_size):\n",
    "        images.append(image)\n",
    "        if 'paligemma' in model_id.lower():\n",
    "            prompt = \"This is a photo of a\"\n",
    "        elif 'moondream' in model_id.lower():\n",
    "            prompt = \"This is a photo of a\"\n",
    "        elif 'internvl2' in model_id.lower():\n",
    "            pixel_value = processor.load_image(image_url, max_num=24).to(torch.bfloat16).cuda()\n",
    "            num_patches_list.append(pixel_value.size(0))\n",
    "            if pixel_values is None:\n",
    "                pixel_values = pixel_value\n",
    "            else:\n",
    "                pixel_values = torch.cat((pixel_values, pixel_value), dim=0)\n",
    "            prompt = \"<image>\\nThis is a photo of a\"\n",
    "        elif 'minicpm' in model_id.lower():\n",
    "            prompt = [{'role': 'user', 'content': \"nThis is a photo of a\"}]\n",
    "        else:\n",
    "            resulting_messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [{\"type\": \"image\"}] + [{\"type\": \"text\", \"text\": \"This is a photo of a\"}]\n",
    "                }\n",
    "            ]\n",
    "            prompt = processor.apply_chat_template(resulting_messages, add_generation_prompt=True)\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    if 'moondream' in model_id.lower() or 'internvl2' in model_id.lower() or 'minicpm' in model_id.lower():\n",
    "        inputs = []\n",
    "        prefill_tokens_per_sequence = -1\n",
    "    else:\n",
    "        inputs = processor(text=prompts, images=images, return_tensors=\"pt\",\n",
    "                       padding=True).to(device).to(torch.bfloat16)\n",
    "        prefill_tokens_per_sequence = inputs.input_ids.size(1) # this is the input seq len basically\n",
    "    \n",
    "    total_prefill_tokens = prefill_tokens_per_sequence * batch_size\n",
    "\n",
    "    # Measure memory before warmup\n",
    "    memory_before_prefill = get_gpu_memory()\n",
    "    print(f\"Memory before prefill: {memory_before_prefill:.2f} GB\")\n",
    "\n",
    "    with torch.no_grad(): # warmup\n",
    "        for _ in range(10):\n",
    "            if 'moondream' in model_id.lower():\n",
    "                _ = model.batch_answer(images[0], prompt, processor)\n",
    "            elif 'internvl2' in model_id.lower():\n",
    "                _ = model.batch_chat(processor.tokenizer, pixel_values,\n",
    "                        num_patches_list=num_patches_list,\n",
    "                        questions=prompts,\n",
    "                        generation_config=dict(max_new_tokens=10, do_sample=True))\n",
    "            elif 'minicpm' in model_id.lower():\n",
    "                res, context, _ = model.chat(\n",
    "                        image=image[0],\n",
    "                        msgs=prompt,\n",
    "                        context=None,\n",
    "                        tokenizer=processor,\n",
    "                        sampling=True,\n",
    "                        temperature=0.7\n",
    "                    )\n",
    "            else:\n",
    "                _ = model(**inputs)\n",
    "\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        if 'moondream' in model_id.lower():\n",
    "            _ = model.batch_answer(images[0], prompt, processor)\n",
    "        elif 'internvl2' in model_id.lower():\n",
    "            _ = model.batch_chat(processor.tokenizer, pixel_values,\n",
    "                num_patches_list=num_patches_list,\n",
    "                questions=prompts,\n",
    "                generation_config=dict(max_new_tokens=1, do_sample=True))\n",
    "        elif 'minicpm' in model_id.lower():\n",
    "            res, context, _ = model.chat(\n",
    "                    image=image[0],\n",
    "                    msgs=prompt,\n",
    "                    context=None,\n",
    "                    tokenizer=processor,\n",
    "                    sampling=True,\n",
    "                    temperature=0.7\n",
    "                )\n",
    "        else:\n",
    "            output = model(**inputs)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Measure memory after prefill\n",
    "    memory_after_prefill = get_gpu_memory()\n",
    "    print(f\"Memory after prefill: {memory_after_prefill:.2f} GB\")\n",
    "    \n",
    "    prefill_duration = end_time - start_time\n",
    "    prefill_tps = total_prefill_tokens / prefill_duration\n",
    "    print(f\"Prefill tokens per second: {prefill_tps:.2f}\")\n",
    "    print(f\"Prefill memory usage: {memory_after_prefill:.2f} GB\")\n",
    "\n",
    "    gen_token_count_per_sequence = 200\n",
    "    total_gen_tokens = gen_token_count_per_sequence * batch_size\n",
    "    if 'moondream' in model_id.lower() or 'internvl2' in model_id.lower() or 'minicpm' in model_id.lower():\n",
    "        input_ids = inputs\n",
    "    else:\n",
    "        input_ids = inputs.input_ids\n",
    "\n",
    "    # Measure memory before generation\n",
    "    memory_before_gen = get_gpu_memory()\n",
    "    print(f\"Memory before generation: {memory_before_gen:.2f} GB\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    output = []\n",
    "    for _ in range(gen_token_count_per_sequence):\n",
    "        with torch.no_grad():\n",
    "            if 'moondream' in model_id.lower():\n",
    "                _ = model.batch_answer(images[0], prompt, processor)\n",
    "                break\n",
    "            elif 'internvl2' in model_id.lower():\n",
    "                _ = model.batch_chat(processor.tokenizer, pixel_values,\n",
    "                    num_patches_list=num_patches_list,\n",
    "                    questions=prompts,\n",
    "                    generation_config=dict(max_new_tokens=gen_token_count_per_sequence, do_sample=True))\n",
    "                break\n",
    "            elif 'minicpm' in model_id.lower():\n",
    "                res, context, _ = model.chat(\n",
    "                        image=image[0],\n",
    "                        msgs=prompt,\n",
    "                        context=None,\n",
    "                        tokenizer=processor,\n",
    "                        sampling=True,\n",
    "                        temperature=0.7\n",
    "                    )\n",
    "                break\n",
    "            else:\n",
    "                output = model(input_ids)\n",
    "                next_token = torch.argmax(output.logits[:, -1, :], dim=-1, keepdim=True)\n",
    "                input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Measure memory after generation\n",
    "    memory_after_gen = get_gpu_memory()\n",
    "    print(f\"Memory after generation: {memory_after_gen:.2f} GB\")\n",
    "    \n",
    "    gen_duration = end_time - start_time\n",
    "    gen_tps = total_gen_tokens / gen_duration\n",
    "    print(f\"Generated tokens per second: {gen_tps:.2f}\")\n",
    "    print(f\"Generation memory usage: {memory_after_gen:.2f} GB\")\n",
    "    \n",
    "    # Clean up after benchmark\n",
    "    del output, input_ids\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return prefill_tps, gen_tps, base_memory, memory_after_prefill, memory_after_gen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory: 0.00 GB\n",
      "\n",
      "Testing model: openbmb/MiniCPM-V-2 with batch size: 1\n",
      "Current memory: 0.00 GB\n",
      "Max Memory Reserved: 0.00 GB\n",
      "Initial GPU memory usage before loading: 0.00 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b094fcc7365d4a95ab7757b124744c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory usage after loading: 6.53 GB\n",
      "GPU memory usage from loading the model: 6.53 GB\n",
      "Memory before prefill: 6.53 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory after prefill: 6.56 GB\n",
      "Prefill tokens per second: -0.48\n",
      "Prefill memory usage: 6.56 GB\n",
      "Memory before generation: 6.56 GB\n",
      "Memory after generation: 6.56 GB\n",
      "Generated tokens per second: 88.80\n",
      "Generation memory usage: 6.56 GB\n",
      "Max Memory: 7.36 GB\n",
      "Max Memory Reserved: 7.88 GB\n",
      "\n",
      "Final Results:\n",
      "\n",
      "Model openbmb/MiniCPM-V-2 with batch size 1:\n",
      "Base model memory usage: 6.53 GB\n",
      "Prefill TPS: -0.48\n",
      "Generation TPS: 88.80\n",
      "Additional Prefill Memory: 6.56 GB\n",
      "Additional Generation Memory: 6.56 GB\n",
      "Max Memory: 7.36 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import threading\n",
    "\n",
    "model_ids = [\"HuggingFaceTB/SmolVLM-Instruct\", \"Qwen/Qwen2-VL-2B-Instruct\", \"HuggingFaceM4/Idefics3-8B-Llama3\", \"OpenGVLab/InternVL2-2B\", \"google/paligemma-3b-mix-448\", \"vikhyatk/moondream2\",\n",
    "             \"openbmb/MiniCPM-V-2\"]\n",
    "\n",
    "model_ids = [\"openbmb/MiniCPM-V-2\"]\n",
    "# For MiniCPM to work we need to change the code as described here: https://huggingface.co/openbmb/MiniCPM-V-2/discussions/23\n",
    "\n",
    "print(f\"Initial memory: {torch.cuda.max_memory_allocated() / (1024**3):.2f} GB\")\n",
    "\n",
    "stats = {}\n",
    "for model_id in model_ids:\n",
    "    for batch_size in [1]:#, 4]:#, 32, 64]:\n",
    "        print(f\"\\nTesting model: {model_id} with batch size: {batch_size}\")\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        torch.cuda.torch.cuda.reset_peak_memory_stats()\n",
    "        print(f\"Current memory: {torch.cuda.max_memory_allocated() / (1024**3):.2f} GB\")\n",
    "        print(f\"Max Memory Reserved: {torch.cuda.max_memory_reserved() / (1024**3):.2f} GB\")\n",
    "\n",
    "        prefill_tps, gen_tps, base_memory, prefill_mem, gen_mem = benchmark_vlm(model_id, batch_size)\n",
    "        stats[(model_id, batch_size)] = {\n",
    "            \"prefill_tps\": prefill_tps, \n",
    "            \"gen_tps\": gen_tps,\n",
    "            \"base_memory_gb\": base_memory,\n",
    "            \"prefill_memory_gb\": prefill_mem,\n",
    "            \"gen_memory_gb\": gen_mem,\n",
    "            \"max_memory_gb\": torch.cuda.max_memory_allocated() / (1024**3),\n",
    "            \"max_memory_reserved_gb\": torch.cuda.max_memory_reserved() / (1024**3)\n",
    "        }\n",
    "        print(f\"Max Memory: {torch.cuda.max_memory_allocated() / (1024**3):.2f} GB\")\n",
    "        print(f\"Max Memory Reserved: {torch.cuda.max_memory_reserved() / (1024**3):.2f} GB\")\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "# Print final results\n",
    "print(\"\\nFinal Results:\")\n",
    "for (model_id, batch_size), metrics in stats.items():\n",
    "    print(f\"\\nModel {model_id} with batch size {batch_size}:\")\n",
    "    print(f\"Base model memory usage: {base_memory:.2f} GB\")\n",
    "    print(f\"Prefill TPS: {metrics['prefill_tps']:.2f}\")\n",
    "    print(f\"Generation TPS: {metrics['gen_tps']:.2f}\")\n",
    "    print(f\"Additional Prefill Memory: {metrics['prefill_memory_gb']:.2f} GB\")\n",
    "    print(f\"Additional Generation Memory: {metrics['gen_memory_gb']:.2f} GB\")\n",
    "    print(f\"Max Memory: {metrics['max_memory_gb']:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some kwargs in processor config are unused and will not have any effect: image_seq_len. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>User:<image>What do we see in this image?\n",
      "Assistant: This image shows a city skyline with prominent landmarks.\n",
      "User:<image>And how about this image?\n",
      "Assistant: In this image, there is a bee on a pink flower.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "from transformers.image_utils import load_image\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load images\n",
    "image1 = load_image(\"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\")\n",
    "image2 = load_image(\"https://huggingface.co/spaces/merve/chameleon-7b/resolve/main/bee.jpg\")\n",
    "\n",
    "# Initialize processor and model\n",
    "processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\")\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"HuggingFaceTB/SmolVLM-Instruct\", torch_dtype=torch.bfloat16\n",
    ").to(DEVICE)\n",
    "\n",
    "# Create input messages\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"What do we see in this image?\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"This image shows a city skyline with prominent landmarks.\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"And how about this image?\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Prepare inputs\n",
    "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(text=prompt, images=[image1, image2], return_tensors=\"pt\")\n",
    "inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "\n",
    "# Generate outputs\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=500)\n",
    "generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "print(generated_texts[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
